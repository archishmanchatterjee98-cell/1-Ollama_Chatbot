# 1-Ollama_Chatbot

Intro: 
A lightweight, customizable chatbot built with Ollama for running large language models (LLMs) locally â€” no cloud dependency, no API keys.

ğŸ” Features
ğŸ¦™ Uses local models like llama2, mistral, etc. via Ollama

ğŸ’¬ Supports multi-turn conversations with chat history

ğŸ§± Built with LangChain and Streamlit for modularity and fast UI


âš¡ Fast inference with minimal system resources

ğŸš€ Tech Stack
Ollama â€” local LLM runtime

LangChain â€” LLM orchestration

Streamlit â€” UI framework

Python 3.10+

ğŸ“¦ Use Cases
Offline AI assistant

Local coding helper

Lightweight developer tool


Prerequisites
Before running this application, ensure the following:

1. Python 3.10+ is installed on your machine.

2. Install required packages via pip or uv also for faster execution:
pip install -r requirements.txt

3. Ollama must be installed and running locally
Refer to https://ollama.com for installation instructions.

4. Models like llama2, gemma2, or mistral should be available locally in Ollama.
  You can pull them using:
  ollama pull llama2
  ollama pull gemma2
  ollama pull mistral

5. The user interface is implemented using [Streamlit](https://streamlit.io/), a powerful and easy-to-use Python framework for building interactive web applications.

- ğŸ”¹ Clean and minimalistic frontend
- ğŸ”¹ Supports real-time interaction with backend logic
- ğŸ”¹ Runs locally in the browser

To launch the Streamlit app, use:
streamlit run app.py
6 .env file (optional but recommended): You may create a .env file in the root directory to manage environment variables (e.g., for future enhancements like API keys or configuration).




